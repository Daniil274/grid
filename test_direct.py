#!/usr/bin/env python3
"""
Direct testing of new modules without going through core package imports.
"""

import asyncio
import sys
import os
import importlib.util
import traceback

def import_module_directly(file_path, module_name):
    """Import a module directly from file path."""
    spec = importlib.util.spec_from_file_location(module_name, file_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module

async def test_agent_profiles_direct():
    """Test agent profiles by importing directly."""
    print("\nüéØ Testing Agent Profiles (Direct Import)...")
    
    try:
        # Import directly without going through core package
        profiles_module = import_module_directly(
            "core/agent_profiles.py", 
            "agent_profiles"
        )
        
        ProfileManager = profiles_module.ProfileManager
        AgentProfile = profiles_module.AgentProfile
        ProfileTemplate = profiles_module.ProfileTemplate
        
        # Test profile creation
        profile_manager = ProfileManager()
        
        # Test getting built-in profile
        file_profile = profile_manager.registry.get_profile("file_worker")
        assert file_profile is not None, "File worker profile should exist"
        assert file_profile.template == ProfileTemplate.FILE_OPERATIONS
        assert "file_read" in file_profile.tools
        
        print(f"‚úÖ Found profile: {file_profile.name}")
        print(f"   Template: {file_profile.template}")
        print(f"   Tools: {file_profile.tools[:3]}...")  # Show first 3 tools
        print(f"   Guardrails: {file_profile.guardrails}")
        
        # Test profile application
        agent_config = {
            "name": "Test Agent",
            "model": "gpt-4", 
            "tools": ["custom_tool"]
        }
        
        enhanced_config = profile_manager.apply_profile(agent_config, "file_worker")
        assert "file_read" in enhanced_config["tools"]
        assert "custom_tool" in enhanced_config["tools"]
        assert "input_validation" in enhanced_config["guardrails"]
        
        print(f"‚úÖ Enhanced config tools: {enhanced_config['tools'][:5]}...")
        print(f"   Guardrails: {enhanced_config['guardrails']}")
        
        # Test custom profile
        custom_profile = AgentProfile(
            name="Custom Analyst",
            template=ProfileTemplate.ANALYSIS,
            capabilities=["data_processing", "visualization"],
            tools=["data_tool", "chart_tool"],
            guardrails=["input_validation"]
        )
        
        profile_manager.registry.register_profile("custom_analyst", custom_profile)
        retrieved = profile_manager.registry.get_profile("custom_analyst")
        assert retrieved.name == "Custom Analyst"
        
        print(f"‚úÖ Custom profile registered: {retrieved.name}")
        
        # Test all built-in profiles
        all_profiles = profile_manager.registry.list_profiles()
        print(f"‚úÖ Total profiles available: {len(all_profiles)}")
        for name, profile in all_profiles.items():
            print(f"   - {name}: {profile.name} ({profile.template.value})")
        
        print("‚úÖ Agent Profiles: All tests passed!")
        return True
        
    except Exception as e:
        print(f"‚ùå Agent Profiles: Test failed - {e}")
        traceback.print_exc()
        return False

async def test_guardrails_direct():
    """Test guardrails by importing directly."""
    print("\nüõ°Ô∏è Testing Guardrails (Direct Import)...")
    
    try:
        guardrails_module = import_module_directly(
            "core/guardrails.py",
            "guardrails"
        )
        
        GuardrailManager = guardrails_module.GuardrailManager
        InputValidationGuardrail = guardrails_module.InputValidationGuardrail
        PathSafetyGuardrail = guardrails_module.PathSafetyGuardrail
        CodeSafetyGuardrail = guardrails_module.CodeSafetyGuardrail
        OutputSanitizationGuardrail = guardrails_module.OutputSanitizationGuardrail
        
        # Test input validation
        input_guardrail = InputValidationGuardrail(max_length=100, min_length=5)
        
        # Valid input
        result = await input_guardrail.validate_input("This is a valid input message")
        assert result.is_valid
        print("‚úÖ Valid input passed validation")
        
        # Invalid input (too short)
        result = await input_guardrail.validate_input("Hi")
        assert not result.is_valid
        print(f"‚úÖ Short input rejected: {result.message}")
        
        # Invalid input (too long)
        result = await input_guardrail.validate_input("x" * 200)
        assert not result.is_valid
        print(f"‚úÖ Long input rejected: {result.message}")
        
        # Test path safety
        path_guardrail = PathSafetyGuardrail(allowed_paths=["."])
        
        result = await path_guardrail.validate_input("Read file ./data.txt")
        if not result.is_valid:
            print(f"‚ö†Ô∏è Path validation issue: {result.message}")
            # Test with a simpler path
            result = await path_guardrail.validate_input("Read file data.txt")
        print(f"‚úÖ Path validation result: {result.is_valid} - {result.message}")
        
        result = await path_guardrail.validate_input("Delete file ../../../etc/passwd")
        assert not result.is_valid
        print(f"‚úÖ Unsafe path rejected: {result.message}")
        
        # Test code safety
        code_guardrail = CodeSafetyGuardrail()
        
        result = await code_guardrail.validate_input("print('Hello World')")
        assert result.is_valid
        print("‚úÖ Safe code passed validation")
        
        result = await code_guardrail.validate_input("eval(user_input)")
        assert not result.is_valid
        print(f"‚úÖ Dangerous code rejected: {result.message}")
        
        # Test output sanitization
        output_guardrail = OutputSanitizationGuardrail()
        
        result = await output_guardrail.validate_output("Your email is user@example.com")
        assert "[EMAIL_REDACTED]" in result.modified_content
        print(f"‚úÖ Email sanitized: {result.modified_content}")
        
        # Test guardrail manager
        manager = GuardrailManager()
        
        print(f"‚úÖ Guardrail manager initialized with {len(manager._guardrails)} guardrails:")
        for name, guardrail in manager._guardrails.items():
            print(f"   - {name}: {guardrail.description}")
        
        # Test manager validation
        valid, results = await manager.validate_input(
            "Read the file data.txt",
            ["input_validation", "path_safety"]
        )
        assert valid
        print(f"‚úÖ Manager validation passed: {len(results)} guardrails checked")
        
        print("‚úÖ Guardrails: All tests passed!")
        return True
        
    except Exception as e:
        print(f"‚ùå Guardrails: Test failed - {e}")
        traceback.print_exc()
        return False

async def test_evaluations_direct():
    """Test evaluation system by importing directly."""
    print("\nüìä Testing Evaluation System (Direct Import)...")
    
    try:
        evals_module = import_module_directly(
            "core/evals.py",
            "evals"
        )
        
        EvalManager = evals_module.EvalManager
        EvalSuite = evals_module.EvalSuite
        TestCase = evals_module.TestCase
        AccuracyEvaluator = evals_module.AccuracyEvaluator
        RelevanceEvaluator = evals_module.RelevanceEvaluator
        CompletenessEvaluator = evals_module.CompletenessEvaluator
        SafetyEvaluator = evals_module.SafetyEvaluator
        EfficiencyEvaluator = evals_module.EfficiencyEvaluator
        
        # Test accuracy evaluator
        evaluator = AccuracyEvaluator()
        
        test_case = TestCase(
            id="test_1",
            input="What is 2+2?",
            expected_output="The answer is 4"
        )
        
        # Perfect match
        result = await evaluator.evaluate(test_case, "The answer is 4")
        assert result.score == 1.0
        print(f"‚úÖ Perfect match score: {result.score}")
        
        # Partial match
        result = await evaluator.evaluate(test_case, "The result is 4")
        print(f"‚úÖ Partial match score: {result.score:.2f}")
        assert 0.5 < result.score < 1.0
        
        # Test relevance evaluator
        relevance_evaluator = RelevanceEvaluator()
        
        test_case_2 = TestCase(
            id="test_2", 
            input="Explain file operations in Python",
            expected_behavior="Should explain file operations"
        )
        
        relevant_response = "File operations in Python include reading, writing, and manipulating files using built-in functions"
        result = await relevance_evaluator.evaluate(test_case_2, relevant_response)
        print(f"‚úÖ Relevance score for good response: {result.score:.2f}")
        assert result.score > 0.5
        
        irrelevant_response = "The weather is nice today"
        result = await relevance_evaluator.evaluate(test_case_2, irrelevant_response)
        print(f"‚úÖ Relevance score for bad response: {result.score:.2f}")
        assert result.score < 0.5
        
        # Test safety evaluator
        safety_evaluator = SafetyEvaluator()
        
        safe_response = "Here's how to read a file safely in Python"
        result = await safety_evaluator.evaluate(TestCase("safety_1", "test", expected_behavior="safe"), safe_response)
        print(f"‚úÖ Safety score for safe response: {result.score:.2f}")
        assert result.score == 1.0
        
        unsafe_response = "Use eval() to execute this: rm -rf /"
        result = await safety_evaluator.evaluate(TestCase("safety_2", "test", expected_behavior="safe"), unsafe_response)
        print(f"‚úÖ Safety score for unsafe response: {result.score:.2f}")
        assert result.score < 1.0
        
        # Test evaluation suite
        eval_manager = EvalManager()
        
        suite = EvalSuite("test_suite", [
            TestCase("test_1", "What is Python?", expected_output="Python is a programming language"),
            TestCase("test_2", "How to read files?", expected_behavior="Should explain file reading")
        ])
        
        eval_manager.register_suite(suite)
        print(f"‚úÖ Registered evaluation suite with {len(suite.test_cases)} test cases")
        
        # Mock agent runner
        async def mock_agent_runner(agent_key: str, input_msg: str) -> str:
            if "Python" in input_msg:
                return "Python is a powerful programming language used for many applications"
            elif "files" in input_msg:
                return "Use the open() function to read files in Python"
            return "I don't understand the question"
        
        # Run evaluation
        result = await eval_manager.run_suite("test_suite", mock_agent_runner, "test_agent")
        
        print(f"‚úÖ Evaluation completed:")
        print(f"   Overall score: {result.overall_score:.2f}")
        print(f"   Passed: {result.passed_count}/{len(result.results)}")
        print(f"   Duration: {result.total_duration:.2f}s")
        
        # Test performance summary
        summary = eval_manager.get_agent_performance_summary("test_agent")
        print(f"‚úÖ Performance summary generated for {summary.get('total_evaluations', 0)} evaluations")
        
        print("‚úÖ Evaluation System: All tests passed!")
        return True
        
    except Exception as e:
        print(f"‚ùå Evaluation System: Test failed - {e}")
        traceback.print_exc()
        return False

async def test_tool_registry_direct():
    """Test tool registry by importing directly."""
    print("\nüîß Testing Tool Registry (Direct Import)...")
    
    try:
        registry_module = import_module_directly(
            "core/tool_registry.py",
            "tool_registry"
        )
        
        ToolRegistry = registry_module.ToolRegistry
        ToolBuilder = registry_module.ToolBuilder
        ToolType = registry_module.ToolType
        
        registry = ToolRegistry()
        
        # Test function tool registration
        def simple_calculator(a: int, b: int) -> int:
            return a + b
        
        registry.register_function_tool(
            "calculator",
            simple_calculator,
            description="Simple calculator tool",
            category="math",
            tags=["calculator", "math"]
        )
        
        tool = registry.get_tool("calculator")
        assert tool is not None
        print(f"‚úÖ Registered function tool: {tool.metadata.name}")
        print(f"   Category: {tool.metadata.category}")
        print(f"   Tags: {tool.metadata.tags}")
        
        # Test tool builder
        builder = ToolBuilder(registry)
        
        def text_processor(input_text: str) -> str:
            return f"Processed: {input_text}"
        
        tool = (builder
                .name("text_processor")
                .description("Processes text input")
                .category("text")
                .tags("processing", "text")
                .function(text_processor)
                .build())
        
        assert tool.metadata.name == "text_processor"
        print(f"‚úÖ Built tool with fluent interface: {tool.metadata.name}")
        
        # Test composite tool
        def step1(input: str) -> str:
            return f"Step1: {input}"
        
        def step2(input: str) -> str:
            return f"Step2: {input}"
        
        registry.register_function_tool("step1", step1)
        registry.register_function_tool("step2", step2)
        
        registry.register_composite_tool(
            "two_step_process",
            ["step1", "step2"],
            description="Two-step processing chain"
        )
        
        composite = registry.get_tool("two_step_process")
        assert composite is not None
        print(f"‚úÖ Created composite tool: {composite.metadata.name}")
        print(f"   Dependencies: {composite.metadata.dependencies}")
        
        # Test tool statistics
        stats = registry.get_tool_statistics()
        print(f"‚úÖ Tool registry statistics:")
        print(f"   Total tools: {stats['total_tools']}")
        print(f"   By type: {stats['by_type']}")
        print(f"   By category: {stats['by_category']}")
        
        # Test tool validation
        valid, missing = registry.validate_tool_chain(["step1", "step2", "nonexistent"])
        assert not valid
        assert "nonexistent" in missing
        print(f"‚úÖ Tool chain validation detected missing: {missing}")
        
        # Test getting tools by category
        math_tools = registry.get_tools_by_category("math")
        assert "calculator" in math_tools
        print(f"‚úÖ Found {len(math_tools)} math tools: {math_tools}")
        
        # Test getting tools by tags
        processing_tools = registry.get_tools_by_tags(["processing"])
        assert "text_processor" in processing_tools
        print(f"‚úÖ Found {len(processing_tools)} processing tools: {processing_tools}")
        
        print("‚úÖ Tool Registry: All tests passed!")
        return True
        
    except Exception as e:
        print(f"‚ùå Tool Registry: Test failed - {e}")
        traceback.print_exc()
        return False

async def main():
    """Run all direct import tests."""
    print("üî¨ Testing Grid System Enhancements (Direct Import)")
    print("=" * 60)
    
    test_results = []
    
    # Run all direct tests
    test_results.append(await test_agent_profiles_direct())
    test_results.append(await test_guardrails_direct())
    test_results.append(await test_evaluations_direct())
    test_results.append(await test_tool_registry_direct())
    
    # Summary
    passed = sum(test_results)
    total = len(test_results)
    
    print("\n" + "=" * 60)
    print(f"üìä Direct Import Test Summary: {passed}/{total} components working")
    
    if passed == total:
        print("üéâ All new components are fully functional!")
        print("\nüìã Tested Features:")
        print("  ‚úÖ Agent Profiles - Template system and configuration merging")
        print("  ‚úÖ Guardrails - Input/output validation and sanitization")
        print("  ‚úÖ Evaluation System - Quality assessment and benchmarking")
        print("  ‚úÖ Tool Registry - Centralized tool management and composition")
        print("\nüöÄ Ready for production integration!")
        return 0
    else:
        print(f"‚ö†Ô∏è {total - passed} components need attention.")
        return 1

if __name__ == "__main__":
    try:
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\n‚ùå Tests interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\nüí• Unexpected error: {e}")
        traceback.print_exc()
        sys.exit(1)